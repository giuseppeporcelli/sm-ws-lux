{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMakerâ€™s implementation of the linear learner built-in algorithm by building a  basic binary classification model for a predictive maintenance use-case.\n",
    "\n",
    "The implementation is provided for educational purposes only and does not take into account several optimizations, with the aim to keep it simple and make it very easy to follow during a lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and choosing the AWS Region and AWS Role we will use.\n",
    "Also, we need to change the username that is also the prefix of the bucket that will contain the wind turbine training data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "\n",
    "# Replace username placeholder.\n",
    "username = '[username]'\n",
    "bucket_name = '{0}-sm-workshop-lux'.format(username)\n",
    "prefix = '02'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preparation</h2>\n",
    "\n",
    "We first copy the dataset from the public S3 bucket storing the data to your bucket and then to the notebook instance. After running the cell below, you can optionally check that the file was downloaded to the notebook instance throught the Jupyter notebook file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'windturbine_data.csv'\n",
    "}\n",
    "\n",
    "file_name = 'windturbine_data.csv'\n",
    "file_key = '{0}/data/{1}'.format(prefix, file_name)\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)\n",
    "s3.Bucket(bucket_name).download_file(file_key, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "df = pandas.read_csv(file_name)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some descriptive statistics for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok = df[df['breakdown'] == 0]\n",
    "print('Number of positive examples: ' + str(df_ok.shape[0]))\n",
    "\n",
    "df_nok = df[df['breakdown'] == 1]\n",
    "print('Number of negative examples: ' + str(df_nok.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the input file in training and test files (80/20) and we have to swap columns as the Amazon SageMaker linear learner algorithm expects the target variable to be stored in the first column (the target variable is the last one in the input data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = df['breakdown']\n",
    "df.drop(labels=['breakdown'], axis=1, inplace = True)\n",
    "df.insert(0, 'breakdown', target_column)\n",
    "\n",
    "train_set = df[:800000]\n",
    "test_set = df[800000:]\n",
    "\n",
    "train_set.to_csv('windturbine_data_train.csv', header=False, index=False)\n",
    "test_set.to_csv('windturbine_data_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now upload the transformed files back to S3 as it is the storage that Amazon SageMaker will expect to find training data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "target_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "with open('windturbine_data_train.csv', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/data/train/windturbine_data_train.csv'.format(prefix))\n",
    "    \n",
    "with open('windturbine_data_test.csv', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/data/test/windturbine_data_test.csv'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Training</h2>\n",
    "\n",
    "In order to start training, we need to specify the location of the docker container that will be used for training.\n",
    "Docker Registry paths for Amazon algorithms are specified here: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "\n",
    "By the way, we can use a utility function of the Amazon SageMaker Python SDK to get the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner', repo_version=\"latest\")\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training, by specifying the input and output settings and the required hyperparameters.\n",
    "You can find the list of the supported hyperparameters for the linear learner algorithm here: https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html.\n",
    "\n",
    "You can also try running the following cell multiple times changing hyperparameters or other settings like the number of instances to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "output_location = 's3://{0}/{1}/output'.format(bucket_name, prefix)\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.2xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    base_job_name='pred-main-ll-{0}'.format(username))\n",
    "\n",
    "est.set_hyperparameters(feature_dim=10,\n",
    "                        num_models=1,\n",
    "                        predictor_type='binary_classifier',\n",
    "                        mini_batch_size=200,\n",
    "                        normalize_data=True,\n",
    "                        normalize_label=False,\n",
    "                        unbias_data=True,\n",
    "                        unbias_label=False)\n",
    "\n",
    "train_config = sagemaker.session.s3_input('s3://{0}/{1}/data/train/'.format(\n",
    "    bucket_name, prefix), content_type='text/csv')\n",
    "test_config = sagemaker.session.s3_input('s3://{0}/{1}/data/test/'.format(\n",
    "    bucket_name, prefix), content_type='text/csv')\n",
    "\n",
    "est.fit({'train': train_config, 'test': test_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Deployment</h2>\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (inferences) from the model dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'pred-main-ll-{0}-'.format(username) + str(int(time.time()))\n",
    "pred = est.deploy(initial_instance_count=1, \n",
    "                  endpoint_name=endpoint_name,\n",
    "                  instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "# Uncomment the following line to connect to an existing endpoint.\n",
    "# pred = RealTimePredictor('[endpoint-name]')\n",
    "\n",
    "pred.content_type = 'text/csv'\n",
    "pred.serializer = csv_serializer\n",
    "pred.deserializer = json_deserializer\n",
    "\n",
    "test_values = [6,90,61,49,28,82,35,7,61,6]\n",
    "result = pred.predict(test_values)\n",
    "print(result)\n",
    "\n",
    "test_values = [9,20,56,39,15,38,38,10,30,5]\n",
    "result = pred.predict(test_values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleanup</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have completed the experimentation, we can delete the real-time endpoint to avoid incurring in unexpected charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
