{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMakerâ€™s built-in SKLearn container to build a basic binary classification model for a predictive maintenance use-case.\n",
    "\n",
    "The implementation is provided for educational purposes only and does not take into account several optimizations, with the aim to keep it simple and make it very easy to follow during a lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and choosing the AWS Region and AWS Role we will use.\n",
    "Also, we need to change the username that is also the prefix of the bucket that will contain the wind turbine training data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "\n",
    "# Replace username placeholder.\n",
    "username = '[username]'\n",
    "bucket_name = '{0}-sm-workshop-lux'.format(username)\n",
    "prefix = '04'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preparation</h2>\n",
    "\n",
    "We first copy the dataset from the public S3 bucket storing the data to your bucket and then to the notebook instance. After running the cell below, you can optionally check that the file was downloaded to the notebook instance throught the Jupyter notebook file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'windturbine_data.csv'\n",
    "}\n",
    "\n",
    "file_name = 'windturbine_data.csv'\n",
    "file_key = '{0}/data/{1}'.format(prefix, file_name)\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)\n",
    "s3.Bucket(bucket_name).download_file(file_key, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "df = pandas.read_csv('windturbine_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some descriptive statistics for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok = df[df['breakdown'] == 0]\n",
    "print('Number of positive examples: ' + str(df_ok.shape[0]))\n",
    "\n",
    "df_nok = df[df['breakdown'] == 1]\n",
    "print('Number of negative examples: ' + str(df_nok.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the input file in training and test files (80/20) to store the target variable in the first column for convenience (the target variable is the last one in the input data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = df['breakdown']\n",
    "df.drop(labels=['breakdown'], axis=1, inplace = True)\n",
    "df.insert(0, 'breakdown', target_column)\n",
    "\n",
    "train_set = df[:800000]\n",
    "val_set = df[800000:]\n",
    "\n",
    "train_set.to_csv('windturbine_data_train.csv', header=False, index=False)\n",
    "val_set.to_csv('windturbine_data_val.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now upload the transformed files back to S3 as it is the storage that Amazon SageMaker will expect to find training data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "target_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "with open('windturbine_data_train.csv', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/data/train/windturbine_data_train.csv'.format(prefix))\n",
    "    \n",
    "with open('windturbine_data_val.csv', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/data/val/windturbine_data_val.csv'.format(prefix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Training</h2>\n",
    "\n",
    "We are now ready to run the training using the Amazon SageMaker SKLearn built-in container. First let's have a look at the script defining our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'pred_main_sklearn_script.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the training using the SKLearn estimator object of the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "output_location = 's3://{0}/{1}/output'.format(bucket_name, prefix)\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, prefix)\n",
    "\n",
    "est = SKLearn(\n",
    "    entry_point='pred_main_sklearn_script.py',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c5.2xlarge\",\n",
    "    output_path=output_location,\n",
    "    base_job_name='pred-main-skl-{0}'.format(username),\n",
    "    code_location = code_location,\n",
    "    hyperparameters={'max_leaf_nodes': 5, 'max_depth': 3})\n",
    "\n",
    "inputs = {'train': 's3://{0}/{1}/data/train/'.format(bucket_name, prefix),\n",
    " 'val': 's3://{0}/{1}/data/val/'.format(bucket_name, prefix)}\n",
    "\n",
    "est.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Deployment</h2>\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (inferences) from the model dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'pred-main-skl-{0}-'.format(username) + str(int(time.time()))\n",
    "pred = est.deploy(initial_instance_count=1, \n",
    "                  instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "# Uncomment the following line to connect to an existing endpoint.\n",
    "# pred = RealTimePredictor('[endpoint-name]')\n",
    "\n",
    "test_values = [[6,56,61,49,28,82,35,7,61,6]]\n",
    "result = pred.predict(test_values)\n",
    "print(result)\n",
    "\n",
    "test_values = [[9,20,56,39,15,38,38,10,30,5]]\n",
    "result = pred.predict(test_values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleanup</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have completed the experimentation, we can delete the real-time endpoint to avoid incurring in unexpected charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
