{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Training</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Amazon SageMaker built-in Linear Learner algorithm to train a binary classification model, using the pre-processed data generated in step 1.\n",
    "\n",
    "First let's take a look at our preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "\n",
    "# Replace username placeholder.\n",
    "username = '[username]'\n",
    "bucket_name = '{0}-sm-workshop-lux'.format(username)\n",
    "prefix = '05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "file_name = 'windturbine_raw_data.csv.out'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket_name).download_file('{0}/data-bt/{1}'.format(prefix, file_name), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "df = pandas.read_csv(file_name, header=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training and test sets. and then copy back to Amazon S3 to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = df[:800000]\n",
    "test_set = df[800000:]\n",
    "\n",
    "train_set.to_csv('windturbine_data_train.csv', header=False, index=False)\n",
    "test_set.to_csv('windturbine_data_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "target_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "with open('windturbine_data_train.csv', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/data-bt/train/windturbine_data_train.csv'.format(prefix))\n",
    "    \n",
    "with open('windturbine_data_test.csv', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/data-bt/test/windturbine_data_test.csv'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start training, we need to specify the location of the docker container that will be used for training.\n",
    "Docker Registry paths for Amazon algorithms are specified here: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "\n",
    "By the way, we can use a utility function of the Amazon SageMaker Python SDK to get the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner', repo_version=\"latest\")\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training, by specifying the input and output settings and the required hyperparameters. You can find the list of the supported hyperparameters for the linear learner algorithm here: https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html.\n",
    "\n",
    "You can also try running the following cell multiple times changing hyperparameters or other settings like the number of instances to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "output_location = 's3://{0}/{1}/output'.format(bucket_name, prefix)\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c5.4xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    base_job_name='pred-main-train-ll-{0}'.format(username))\n",
    "\n",
    "est.set_hyperparameters(feature_dim=28,\n",
    "                        predictor_type='binary_classifier',\n",
    "                        mini_batch_size=200,\n",
    "                        normalize_data=False,\n",
    "                        normalize_label=False,\n",
    "                        unbias_data=False,\n",
    "                        unbias_label=False)\n",
    "\n",
    "train_config = sagemaker.session.s3_input('s3://{0}/{1}/data-bt/train/'.format(\n",
    "    bucket_name, prefix), content_type='text/csv')\n",
    "test_config = sagemaker.session.s3_input('s3://{0}/{1}/data-bt/test/'.format(\n",
    "    bucket_name, prefix), content_type='text/csv')\n",
    "\n",
    "est.fit({'train': train_config, 'test': test_config })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
