{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker and SKLearn to pre-process a purpose-built wind turbine dataset to simulate a predictive maintenance use-case.\n",
    "\n",
    "The implementation is provided for educational purposes only and does not take into account certain optimizations, with the aim to keep it simple and make it very easy to follow during a lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and choosing the AWS Region and AWS Role we will use.\n",
    "Also, we need to change the bucket_name to the bucket containing the wind turbine training data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "\n",
    "# Replace username placeholder.\n",
    "username = '[username]'\n",
    "bucket_name = '{0}-sm-workshop-lux'.format(username)\n",
    "prefix = '05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Exploration</h2>\n",
    "\n",
    "We first copy the dataset from the public S3 bucket storing the data to your bucket and then to the notebook instance. After running the cell below, you can optionally check that the file was downloaded to the notebook instance throught the Jupyter notebook file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'windturbine_raw_data.csv'\n",
    "}\n",
    "\n",
    "file_name = 'windturbine_raw_data.csv'\n",
    "file_key = '{0}/data/{1}'.format(prefix, file_name)\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)\n",
    "s3.Bucket(bucket_name).download_file(file_key, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('windturbine_raw_data.csv', header=None)\n",
    "df.columns = ['turbine_id', 'turbine_type', 'wind_speed', 'RPM_blade', 'oil_temperature', 'oil_level', 'temperature', \n",
    "              'humidity', 'vibrations_frequency', 'pressure', 'wind_direction', 'breakdown']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some descriptive statistics for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok = df[df['breakdown'] == 'yes']\n",
    "print('Number of positive examples: ' + str(df_ok.shape[0]))\n",
    "\n",
    "df_nok = df[df['breakdown'] == 'no']\n",
    "print('Number of negative examples: ' + str(df_nok.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.turbine_type.isnull()).turbine_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize our findings:\n",
    "<ul>\n",
    "    <li><b>turbine_id</b> is a string identifier, that we choose to preserve in the model and we need to encode.</li>\n",
    "    <li><b>turbine_type</b> is a categorical attribute, and has some missing values. More specifically, all values for turbine TID006 are missing. In this specific case we can choose to replace the value with a constant.</li>\n",
    "    <li><b>oil_temperature</b> is a numeric attribute, and has some missing values.</li>\n",
    "    <li><b>wind_direction</b> is a categorical string attribute, that we need to encode.</li>\n",
    "    <li><b>breakdown</b> is our target variable, that we need to encode.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do preprocessing of our data. We will use the Amazon SageMaker built-in SKLearn container to do this, with a script as an entry point. The script is very similar to a script you might run outside of SageMaker, but you can access useful properties about the SageMaker environment through various environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize '1-predmain-expprep-sklearn-script.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "entry_point = '1-predmain-expprep-sklearn-script.py'\n",
    "output_location = 's3://{0}/{1}/output'.format(bucket_name, prefix)\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, prefix)\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=entry_point,\n",
    "    role=role,\n",
    "    output_path=output_location,\n",
    "    code_location=code_location,\n",
    "    base_job_name='pred-main-prep-skl-{0}'.format(username),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m5.2xlarge\")\n",
    "\n",
    "preprocessing_input = sagemaker.session.s3_input(\n",
    "    's3://{0}/{1}/data/'.format(bucket_name, prefix), content_type='text/csv')\n",
    "\n",
    "sklearn_preprocessor.fit({'prep': preprocessing_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch Transform</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model has been fit, we can use Amazon SageMaker Batch Transform to transform our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{0}/{1}/data-bt/'.format(bucket_name, prefix)\n",
    "\n",
    "transformer = sklearn_preprocessor.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    output_path=output_location,\n",
    "    assemble_with = 'Line',\n",
    "    accept='text/csv')\n",
    "    \n",
    "transformer.transform('s3://{0}/{1}/data/'.format(bucket_name, prefix), \n",
    "                      content_type='text/csv', split_type='Line')\n",
    "\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
